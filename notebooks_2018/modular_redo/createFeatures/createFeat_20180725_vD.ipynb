{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation notebook\n",
    "### Goal is to start with dict of dataframes of wells and a few other pieces and create a single dataframe with all the necessary features for all used wells\n",
    "#### This work is similar to what has been done before but data loading & feature creation is separate and dask is used to speed feature creation\n",
    "##### by Justin Gosses 2018-07-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs used during this notebook are:\n",
    "redo this cell!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import welly\n",
    "from welly import Well\n",
    "import lasio\n",
    "import glob\n",
    "from sklearn import neighbors\n",
    "import pickle\n",
    "import math\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "# import pdvega\n",
    "# import vega\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "welly.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.5\n",
      "0.23.0\n"
     ]
    }
   ],
   "source": [
    "print(dask.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 µs ± 3.38 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "import os\n",
    "env = %env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test results Part 1\n",
    "#### Had to change display options to get this to print in full!\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_colwidth = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_dir = \"../WellsKNN/\"\n",
    "load_dir = \"../loadLAS\"\n",
    "wells_knn_dir = \"../WellsKNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 file imported with a dataframe of the wells that have been loaded, limited to the ones with the right tops and curves and knn neighbor thinkness feature columns added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_load_clean_knn_df = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to load a pickle file of a previously created dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN = pd.read_hdf(wells_knn_dir+wells_load_clean_knn_df, 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After joining on the nearest neighbor dataframe, we can cast the original columns to floats instead of strings which some but not necessarily all might be. \n",
    "When we do this, be careful about variation in depth column name and rename DEPTH and DEPT to DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df_all_wells_wKNN.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable that author might need to change!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for turning everything except UWI, SiteID, and Neighbors obj into a float for easier working with later\n",
    "columns_to_turn_to_floats = ['CALI',\n",
    " 'COND',\n",
    " 'DELT',\n",
    " 'DENS',\n",
    " 'DEPT',\n",
    " 'DEPTH',\n",
    " 'DPHI',\n",
    " 'DPHI:1',\n",
    " 'DPHI:2',\n",
    " 'DT',\n",
    " 'GR',\n",
    " 'GR:1',\n",
    " 'GR:2',\n",
    " 'IL',\n",
    " 'ILD',\n",
    " 'ILD:1',\n",
    " 'ILD:2',\n",
    " 'ILM',\n",
    " 'LITH',\n",
    " 'LLD',\n",
    " 'LLS',\n",
    " 'NPHI',\n",
    " 'PHID',\n",
    " 'PHIN',\n",
    " 'RESD',\n",
    " 'RHOB',\n",
    " 'RT',\n",
    " 'SFL',\n",
    " 'SFLU',\n",
    " 'SN',\n",
    " 'SNP',\n",
    " 'SP',\n",
    " 'McMurray_Base_HorID',\n",
    " 'McMurray_Top_HorID',\n",
    " 'McMurray_Base_DEPTH',\n",
    " 'McMurray_Top_DEPTH',\n",
    " 'McMurray_Base_Qual',\n",
    " 'McMurray_Top_Qual',\n",
    " 'lat',\n",
    " 'lng',\n",
    " 'NN1_McMurray_Top_DEPTH',\n",
    " 'NN1_McMurray_Base_DEPTH',\n",
    " 'NN1_thickness',\n",
    " 'MM_Top_Depth_predBy_NN1thick',\n",
    " 'HorID',\n",
    " 'Pick',\n",
    " 'Quality',\n",
    " 'HorID_paleoz',\n",
    " 'Pick_paleoz',\n",
    " 'Quality_paleoz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_all_wells_wKNN[columns_to_turn_to_floats].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we're going to find some depths! Most of the depths in the wells are from a column called DEPT but there a handfull of wells that use a column called DEPTH. For convience sake, we're going to move the DEPTH values were not NaN to the DEPT column so all depths are in the same column. We're also going to try to replace NaNs in GR with GR:1 and GR:2 where data exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useDiffColNamesToFillInNA(dataframeOfWells,colReplaceList):\n",
    "    \"\"\"\n",
    "    Takes in two arguments,\n",
    "    Argument one is a dataframe of multiple wells\n",
    "    Argument two is a list of lists. Where each sub-list is a  pair of column names. \n",
    "    The right col is used to fill in NANs where they exist in left column.\n",
    "    The function returns a dataframe of wells with the NANs in certain columns replaced based on input arguments.\n",
    "    Example = [[ColA,ColB],[ColF,ColG],[ColZ,ColE]]\n",
    "    \"\"\"\n",
    "    for each in colReplaceList:\n",
    "        print(\"each\",each)\n",
    "        dataframeOfWells[each[0]].fillna(dataframeOfWells[each[1]], inplace=True)\n",
    "    return dataframeOfWells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### list of sub-lists. Items on left are replaced with volumns from right column if left column has a NaN\n",
    "colReplaceList = [['DEPT','DEPTH'],['GR','GR:1'],['GR','GR:2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create new dataframe\n",
    "df_all_wells_wKNN_DEPTHtoDEPT = useDiffColNamesToFillInNA(df_all_wells_wKNN,colReplaceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Look at DEPT to make sure it has gone up, it has!\n",
    "df_all_wells_wKNN_DEPTHtoDEPT.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns for how close a row is (based on depth) from the official pick for that well. \n",
    "### We'll be doing this for Top and Base McMurray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for top McMurray\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'] = df_all_wells_wKNN_DEPTHtoDEPT['Pick'] - df_all_wells_wKNN_DEPTHtoDEPT['DEPT']\n",
    "#### for base McMurray or Top Paleozoic\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'] = df_all_wells_wKNN_DEPTHtoDEPT['Pick_paleoz'] - df_all_wells_wKNN_DEPTHtoDEPT['DEPT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### print a few wells to double check\n",
    "df_all_wells_wKNN_DEPTHtoDEPT[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT SHOULD BE NOTED THAT THE 'correct' PICK DEPTHS IN MANY CASES DO NOT PERFECTLY MATCH THE DEPTHS AVAILABLE IN THE LOGS. \n",
    "### In other words, the pick might be 105 but there is no row with 105.00 depth, only a 104.98 and a 105.02!\n",
    "### This matters for what you count as a correct label!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column for whether a row (based on depth) is within 0.0, +- 5, or >5 from the official pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a column that has a number that symbolizes whether a row is close or not to the 'real' pick\n",
    "#### We'll do this first for Top McMurray and then top Paleozoic, which is basically base McMurray\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopMcMrNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))\n",
    "#### Top paleozoic version\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopPalNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### drop previously created diff_TMcM_Pick_v_DEPT\n",
    "#df_all_wells_wKNN_DEPTHtoDEPT.drop(columns=['diff_Pick_v_DEPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### print a few wells to double check\n",
    "df_all_wells_wKNN_DEPTHtoDEPT.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use thickness from neighor and base to predict top just with that, add as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_df['new_pick']=l_df['Pick']-l_df['DEPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_wells_wKNN['diff_Pick_v_DEPT'] = df_all_wells_wKNN['Pick'] - df_all_wells_wKNN['DEPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT['MM_Top_Depth_predBy_NN1thick'][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Takes MM_Top_Depth_predBy_NN1thick and subtracts depth at that point, returns *absolute* value\n",
    "def NN1_TopMcMDepth_Abs(df,MM_Top_Depth_predBy_NN1thick):\n",
    "    df['DistFrom_NN1_TopDepth_Abs'] = abs(df[MM_Top_Depth_predBy_NN1thick] - df['DEPT'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM = NN1_TopMcMDepth_Abs(df_all_wells_wKNN_DEPTHtoDEPT,'MM_Top_Depth_predBy_NN1thick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we'll create a variety of calculated features based on well log numbers at, above, below, and around each depth point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The difficult thing about creating features based on windows within a well when you have multiple wells stacked in a dataframe is that sometimes that window from one well goes into the next well.\n",
    "\n",
    "#### To get around that, we're going create a column that says the distance from the top of the well and another column that says the distance form the bottom of the well. When a row's distance from top or bottom is greater than 1/2 the max window size, we'll just use proceed as normal. When the distance between that row's depth and top or bottom is less than 1/2 the max window size, we'll ....................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(-1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopOfWell'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM[NewWell] == True,,\n",
    "\n",
    "\n",
    "# df['elderly'] = np.where(df['age']>=50, 'yes', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopOfWellRowsOnly = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM.loc[df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] == True]\n",
    "BottomOfWellRowsOnly = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM.loc[df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename depth to top and bottom depths , delete all other columns\n",
    "TopOfWellRowsOnly = TopOfWellRowsOnly[['UWI','DEPT']]\n",
    "TopOfWellRowsOnly['TopWellDept'] = TopOfWellRowsOnly['DEPT']\n",
    "TopOfWellRowsOnly.drop(['DEPT'],axis=1, inplace=True)\n",
    "#### same thing for bottom\n",
    "BottomOfWellRowsOnly = BottomOfWellRowsOnly[['UWI','DEPT']]\n",
    "BottomOfWellRowsOnly['BotWellDept'] = BottomOfWellRowsOnly['DEPT']\n",
    "BottomOfWellRowsOnly.drop(['DEPT'],axis=1, inplace=True)\n",
    "#### merge these two small dataframes\n",
    "TopAndBottomOfWellRowsOnly = pd.merge(TopOfWellRowsOnly, BottomOfWellRowsOnly, on='UWI')\n",
    "#### merge with larger dataframe\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM = pd.merge(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM, TopAndBottomOfWellRowsOnly, on='UWI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a col for distance from row to top of well\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['DEPT'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopWellDept']\n",
    "\n",
    "#### Create a col for distance from row to bottom of well\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['BotWellDept'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['DEPT']\n",
    "\n",
    "#### Create col for well total thickness measured\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['WellThickness'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['BotWellDept'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopWellDept']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This adds a column that says whether a row is closer to the bottm or the top of the well\n",
    "#### This is useful for doing creation of features of rolling windows where you want to avoid going into another well stacked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This adds a column that says whether a row is closer to the bottm or the top of the well\n",
    "#### This is useful for doing creation of features of rolling windows where you want to avoid going into another well stacked above.\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closerToBotOrTop'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], 'FromTopWell', 'FromBotWell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist']/0.25\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing dataframe to pickle file before doing main feature creation step that uses curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dumping dict of data frame to pickle file\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM\n",
    "pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724, open( \"df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the same dataframe from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop =  pd.read_pickle('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is a rewrite of the basic features calculated from the curves\n",
    "### It runs faster than previous Pandas version could be made to run faster, specifically by using `apply` less\n",
    "### It also calculates things in a window around a point, above a point, but not below a point. I have to go back and re-write that code. Sorting is expensive task in Dask, so I don't want to reverse order twice for each feature like I did previously in Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next two lines bring up a Dask client dashboard that will open as a new tab. It provides great insight into what functions are being run by dask, how they run, and which ones are slowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit is only creating features based on curve data within a given well, so we'll read and write pickle files at the start and end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop =  pd.read_pickle('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "test_5 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop.copy()\n",
    "test_5 = dd.from_pandas(test_5, npartitions=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the line below, we pick the curves and windows to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves = ['GR','ILD']\n",
    "windows = [5,7,11,21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function nLargest is used via `apply`, I should probably re-write this to use Dask's Nlargest API but didn't here as the docs imply it might behave slightly differently.\n",
    "### A quick look at the status dashboard in the Dask Client suggests the use of apply takes up maybe 1/4-1/2 of total compute time currently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nLargest(array,nValues):\n",
    "    answer = np.mean(array[np.argsort(array)[-nValues:]])  \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thoughts_seperateRollingAndConditionalIntoTwoDaskProcesses(dd,curves,windows):\n",
    "    \"\"\"\n",
    "    for loop for each combination of parameter for rolling functions\n",
    "    curves = ['GR','ILD']\n",
    "    windows = [5,7,11,21]\n",
    "    directions = [\"around\",\"below\",\"above\"]\n",
    "        #         Not sure the best way to do the 'below' centered rolling in dask as the sort_index is expensive in dask so might be slow!\n",
    "        #       Skipping this for now will come back when not tired. Maybe use shift?\n",
    "    For each column created, check window size vs. allowable window size column, if too small, use single row value from original column\n",
    "    \"\"\"\n",
    "    comboArg_B = [curves,windows]\n",
    "    all_comboArgs_B = list(itertools.product(*comboArg_B))\n",
    "    for eachArgList in all_comboArgs_B:\n",
    "        col = eachArgList[0]\n",
    "        windowSize = eachArgList[1]\n",
    "        #centered = eachArgList[2]\n",
    "        featureName = col+\"_min_\"+str(windowSize)+\"winSize_\"\n",
    "        half_window = int(windowSize/2)\n",
    "        #         quarter_window = int(windowSize/4)\n",
    "\n",
    "        \n",
    "        ### goes through distance to edge and when less than windowSize writes \"too close\" otherwise returns NaN\n",
    "        ### fills in Nan with calculated feature column\n",
    "        ### replaces \"too close\" with NaN\n",
    "        ### replaces NaN with dd[col]\n",
    "        ### overrights original column\n",
    "        \n",
    "        #### MIN\n",
    "        dd[featureName+'dir'+'Around'+'Min'] = dd[col].rolling(windowSize,center=True).min()\n",
    "        dd[featureName+'dir'+'Around'+'Min'] = dd[featureName+'dir'+'Around'+'Min'].where(cond=dd['closTopBotDist'] > half_window, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Above'+'Min'] = dd[col].rolling(windowSize,center=False).min()\n",
    "        dd[featureName+'dir'+'Above'+'Min'] = dd[featureName+'dir'+'Above'+'Min'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "        #### MAX\n",
    "        dd[featureName+'dir'+'Around'+'Max'] = dd[col].rolling(windowSize,center=True).max()\n",
    "        dd[featureName+'dir'+'Around'+'Max'] = dd[featureName+'dir'+'Around'+'Max'].where(cond=dd['closTopBotDist'] > half_window, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Above'+'Max'] = dd[col].rolling(windowSize,center=False).max()\n",
    "        dd[featureName+'dir'+'Above'+'Max'] = dd[featureName+'dir'+'Above'+'Max'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "        #### Mean\n",
    "        dd[featureName+'dir'+'Around'+'Mean'] = dd[col].rolling(windowSize,center=True).mean()\n",
    "        dd[featureName+'dir'+'Around'+'Mean'] = dd[featureName+'dir'+'Around'+'Mean'].where(cond=dd['closTopBotDist'] > half_window, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Above'+'Mean'] = dd[col].rolling(windowSize,center=False).mean()\n",
    "        dd[featureName+'dir'+'Above'+'Mean'] = dd[featureName+'dir'+'Above'+'Mean'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "\n",
    "        ## nLargest\n",
    "        nValues = 5\n",
    "        dd[featureName+'dir'+'Above'+'nLarge'] = dd[col].rolling(windowSize,center=False).apply( lambda x: nLargest(x,nValues),raw=True)  \n",
    "        dd[featureName+'dir'+'Above'+'nLarge'] = dd[featureName+'dir'+'Above'+'nLarge'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Around'+'nLarge'] = dd[col].rolling(windowSize,center=True).apply(lambda x: nLargest(x,nValues),raw=True) \n",
    "        dd[featureName+'dir'+'Around'+'nLarge'] = dd[featureName+'dir'+'Around'+'nLarge'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "    \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "ddf_test5 = thoughts_seperateRollingAndConditionalIntoTwoDaskProcesses(test_5,curves,windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "test5result = ddf_test5.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "test5result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test5result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test5result.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing pandas dataframe to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dumping dict of data frame to pickle file\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726 = test5result\n",
    "pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726, open( \"df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2 =  pd.read_pickle(\"df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pandas dataframe to hdf5 \n",
    "#### Dropping [Neighbors_Obj] col as it is object and can't be written to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Change name and drop [Neighbors_Obj]\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2.drop(['Neighbors_Obj'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write hdf5 to current directory\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3.to_hdf('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore adding features based on map position using widget to draw polygons which are then one-hot encoded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any other features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createFeat_20180707_vA-Copy1.ipynb\r\n",
      "createFeat_20180707_vA.ipynb\r\n",
      "createFeat_20180707_vB.ipynb\r\n",
      "createFeat_20180707_vC.ipynb\r\n",
      "createFeat_20180725_vD.ipynb\r\n",
      "\u001b[34mdask-worker-space\u001b[m\u001b[m\r\n",
      "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724\r\n",
      "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\r\n",
      "dict_of__wells_df_No_features_class3_20180707.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
