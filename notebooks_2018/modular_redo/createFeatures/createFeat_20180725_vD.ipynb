{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation notebook\n",
    "### Goal is to start with dict of dataframes of wells and a few other pieces and create a single dataframe with all the necessary features for all used wells\n",
    "#### This work is similar to what has been done before but data loading & feature creation is separate and dask is used to speed feature creation\n",
    "##### by Justin Gosses 2018-07-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs used during this notebook are:\n",
    "    1. Dict of dataframes of used well created by notebooks in `/loadLAS` directory\n",
    "    2. A dataframe of nearest neighbor information from a notebook found in the `WellsKNN/` directory\n",
    "    3. picks_dic a data dictionary for the pick list below = pd.read_csv('../../SPE_006_originalData/OilSandsDB/PICKS_DIC.TXT',delimiter='\\t')\n",
    "    4. pick list = pd.read_csv('../../SPE_006_originalData/OilSandsDB/PICKS.TXT',delimiter='\\t')\n",
    "    5. well list = pd.read_csv('../../SPE_006_originalData/OilSandsDB/WELLS.TXT',delimiter='\\t')\n",
    "    6. lattitude and longitude for eahc well = pd.read_csv('../../well_lat_lng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import welly\n",
    "from welly import Well\n",
    "import lasio\n",
    "import glob\n",
    "from sklearn import neighbors\n",
    "import pickle\n",
    "import math\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "# import pdvega\n",
    "# import vega\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "welly.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.1\n",
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "print(dask.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.7 µs ± 1.78 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "import os\n",
    "env = %env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test results Part 1\n",
    "#### Had to change display options to get this to print in full!\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.max_colwidth = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_dir = \"../WellsKNN/\"\n",
    "load_dir = \"../loadLAS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you open this notebook fresh and jump to a point below where a pick file is read in, you still need to load everything above! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to load a pickle file of a previously created dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That dataframe merges:\n",
    "1. picks_dic = pd.read_csv('../../SPE_006_originalData/OilSandsDB/PICKS_DIC.TXT',delimiter='\\t')\n",
    "2. picks = pd.read_csv('../../SPE_006_originalData/OilSandsDB/PICKS.TXT',delimiter='\\t')\n",
    "3. wells = pd.read_csv('../../SPE_006_originalData/OilSandsDB/WELLS.TXT',delimiter='\\t')\n",
    "4. gis = pd.read_csv('../../well_lat_lng.csv')\n",
    "\n",
    "### It also excludes any wells that have nulls or zeros for Top McMurray or Base McMurray picks\n",
    "This was done in notebooks: \n",
    "1. notebooks_2018/mapmaking/Map_Exploration_v2-KDtree.ipynb\n",
    "2. notebooks_2018/Test_RUN_2018_02/DataCleaningPrepof_KNN_neighborPickDepth_df_creation_vA_20180210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_df_new_cleaned_plus_nn_wNoNulls =  pd.read_pickle(knn_dir+'/'+'wells_df__NB_KDtreePost062018_vA__NoMcTopLeak_v2.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_df_new_cleaned_plus_nn_wNoNulls.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_df_new_cleaned_plus_nn_wNoNulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wells_df_new_cleaned_plus_nn_wNoNulls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_df_new_cleaned_plus_nn_wNoNulls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This renames the columms of the dataframe above to match previous versions of the feature creation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = wells_df_new_cleaned_plus_nn_wNoNulls\n",
    "# df_new[\"UWI (AGS)\"] = df_new[\"UWI (AGS)_x\"]\n",
    "df_new[\"UWI\"] = df_new[\"UWI\"]\n",
    "df_new[\"HorID\"] = df_new[\"McMurray_Top_HorID\"]\n",
    "df_new[\"Pick\"] = df_new[\"McMurray_Top_DEPTH\"]\n",
    "df_new[\"Quality\"] = df_new[\"McMurray_Top_Qual\"]\n",
    "df_new[\"HorID_paleoz\"] = df_new[\"McMurray_Base_HorID\"]\n",
    "df_new[\"Pick_paleoz\"] = df_new[\"McMurray_Base_DEPTH\"]\n",
    "df_new[\"Quality_paleoz\"] = df_new[\"McMurray_Base_Qual\"]\n",
    "df_new = df_new[[\"SitID\",\"UWI\",\"HorID\",\"Pick\",\"Quality\",\"HorID_paleoz\",\"Pick_paleoz\",\"Quality_paleoz\",'lat','lng','MM_Top_Depth_predBy_NN1thick','NN1_thickness']]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_quality_str = df_new.Quality.unique()\n",
    "print(unique_quality_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Number of unique wells based on UWI\n",
    "len(df_new.UWI.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_test = df_new[['UWI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(any(df_new_test.UWI == '00/11-04-067-03W4/0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're now going to load in all the las files but exclude any that arean't in the dataframe shown above. Additionally, we'll ignore any placed in the `excluded_problem_wells` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndNoFeatures():\n",
    "    count=0\n",
    "    data_df=[]\n",
    "    count_limit =2500\n",
    "    list_of_failed_wells = []\n",
    "    ### dictionary that holds every well as key:value or \"UWI\":df pair\n",
    "    df_w_dict ={}\n",
    "    while count < count_limit:\n",
    "        for file in glob.glob('../../../SPE_006_originalData/OilSandsDB/Logs/*.LAS'):\n",
    "            count+=1\n",
    "            if count > count_limit:\n",
    "                print(\"hit limit of count below file for loop\")\n",
    "                answer = [df_w_dict,list_of_failed_wells]\n",
    "                return answer\n",
    "            else:\n",
    "                l_df = lasio.read(file).df()\n",
    "                str_uwi= file[-23:-4].replace(\"-\", \"/\",1)[:17]+file[-6:-4].replace(\"-\", \"/\",1)\n",
    "                if any(df_new.UWI == str_uwi):\n",
    "                    if df_new[df_new['UWI']==str_uwi]['Quality'].iloc[0] > -1:\n",
    "                        l_df = l_df.reset_index()\n",
    "                        print(\"got to UWI apppend\")\n",
    "                        l_df['UWI'] = str_uwi\n",
    "                        df_w_dict[l_df['UWI'][0]]= l_df\n",
    "                    else:\n",
    "                        print(\"could not find UWI match for the well\")\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "            #print(\"result = \",df_w_dict)\n",
    "    #else: \n",
    "    #    return df_w_dict, list_of_failed_wells\n",
    "    answer = [df_w_dict,list_of_failed_wells]\n",
    "    \n",
    "    return initial_well_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_well_dict = loadAndNoFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer=[df_w_dict,list_of_failed_wells]\n",
    "dict_of_well_df = initial_well_dict[0]\n",
    "list_of_failed_wells = initial_well_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"list_of_failed_wells\",list_of_failed_wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len = \", len(dict_of_well_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"check for well 00/11-04-067-03W4/0 = \",dict_of_well_df['00/11-04-067-03W4/0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dict_of_well_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dict_of_well_df['00/11-04-067-03W4/0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have a dict of Pandas dataframes, were each dataframe is a well, that we will write to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dumping dict of data frame to pickle file\n",
    "dict_wells_df_and_Nofeatures_20180707 = dict_of_well_df\n",
    "pickle.dump(dict_wells_df_and_Nofeatures_20180707, open( \"dict_of__wells_df_No_features_class3_20180707.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step will be to take this dict of dataframes and turn it into a single dataframe or perhaps a dask data frame. Then cycle or cast to add in columns for the information on nearest neighbors from the nearest neighbors dataframe based on a column for UWI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create dataframe from dict \n",
    "2. Add material from KNN dataframe to this dataframe based on UWI\n",
    "3. Go back to original dict of well dataframes and see if a dask dataframe can be created and then add KNN dataframe. \n",
    "4. Test steps (1,2) vs. (3) for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open pickle of dick of well dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictOfWellDf =  pd.read_pickle('dict_of__wells_df_No_features_class3_20180707.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dictOfWellDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnDictOfWellDfs_to_SingleDfOfAllWells(dictOfWellDf):\n",
    "    \"\"\"\n",
    "    Takes in a dict of dataframes, where each dataframe is for a well created by LASIO\n",
    "    and returns a single dataframe of all wells\n",
    "    \"\"\"\n",
    "    # start by creating empty dataframe and list\n",
    "    data_df = pd.DataFrame()\n",
    "    list_of_df = []\n",
    "    # get dict of well data frames into values format\n",
    "    values = dictOfWellDf.values()\n",
    "    # go through each item in values and add to a list\n",
    "    for each in values:\n",
    "        list_of_df.append(each)\n",
    "    # concat the list into a single dataframe\n",
    "    data_df = pd.concat(list_of_df)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_basic = turnDictOfWellDfs_to_SingleDfOfAllWells(dictOfWellDf)\n",
    "print(type(df_all_wells_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_basic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_basic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_basic.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_basic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should probably at some point find the wells with missing major values like GR and either take them out or find out if there is a naming change like GR2 and replace the names so those wells can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_df_new_cleaned_plus_nn_wNoNulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_DfOfAllWells_with_knnDf(df_all_wells_basic,knn_df):\n",
    "    \"\"\"\n",
    "    Takes in 2 arguments, a dataframe of all wells with only basic info \n",
    "    & the dataframe with info on knn neighbor data\n",
    "    and returns a single dataframe that merges the two input dataframes based on UWI column\n",
    "    \"\"\"\n",
    "    df_all_wells_wKNN = pd.merge(df_all_wells_basic, knn_df, on='UWI')\n",
    "    return df_all_wells_wKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df_all_wells_wKNN = combine_DfOfAllWells_with_knnDf(df_all_wells_basic,wells_df_new_cleaned_plus_nn_wNoNulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_wells_wKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_wells_wKNN.UWI.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe of nearest neighbor information had 1926 rows, this now has 1920 unique UWIs.\n",
    "Did some of the wells in import not make it through or where kicked out from later steps????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing as above but for Dask data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnDictOfWellDfs_to_SingleDfOfAllWells(dictOfDF):\n",
    "    \"\"\"\n",
    "    Takes in a dict of dataframes, where each dataframe is for a well created by LASIO\n",
    "    and returns a single dataframe of all wells\n",
    "    \"\"\"\n",
    "    return dask_df_all_wells_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_DfOfAllWells_with_knnDf(dask_df_all_wells_basic,knn_df):\n",
    "    \"\"\"\n",
    "    Takes in 2 arguments, a dataframe of all wells with only basic info \n",
    "    & the dataframe with info on knn neighbor data\n",
    "    and returns a single dataframe that merges the two input dataframes based on UWI column\n",
    "    \"\"\"\n",
    "    return dask_df_all_wells_wKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After joining on the nearest neighbor dataframe, we can cast the original columns to floats instead of strings which some but not necessarily all might be. \n",
    "When we do this, be careful about variation in depth column name and rename DEPTH and DEPT to DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df_all_wells_wKNN.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for turning everything except UWI, SiteID, and Neighbors obj into a float for easier working with later\n",
    "columns_to_turn_to_floats = ['CALI',\n",
    " 'COND',\n",
    " 'DELT',\n",
    " 'DENS',\n",
    " 'DEPT',\n",
    " 'DEPTH',\n",
    " 'DPHI',\n",
    " 'DPHI:1',\n",
    " 'DPHI:2',\n",
    " 'DT',\n",
    " 'GR',\n",
    " 'GR:1',\n",
    " 'GR:2',\n",
    " 'IL',\n",
    " 'ILD',\n",
    " 'ILD:1',\n",
    " 'ILD:2',\n",
    " 'ILM',\n",
    " 'LITH',\n",
    " 'LLD',\n",
    " 'LLS',\n",
    " 'NPHI',\n",
    " 'PHID',\n",
    " 'PHIN',\n",
    " 'RESD',\n",
    " 'RHOB',\n",
    " 'RT',\n",
    " 'SFL',\n",
    " 'SFLU',\n",
    " 'SN',\n",
    " 'SNP',\n",
    " 'SP',\n",
    " 'McMurray_Base_HorID',\n",
    " 'McMurray_Top_HorID',\n",
    " 'McMurray_Base_DEPTH',\n",
    " 'McMurray_Top_DEPTH',\n",
    " 'McMurray_Base_Qual',\n",
    " 'McMurray_Top_Qual',\n",
    " 'lat',\n",
    " 'lng',\n",
    " 'NN1_McMurray_Top_DEPTH',\n",
    " 'NN1_McMurray_Base_DEPTH',\n",
    " 'NN1_thickness',\n",
    " 'MM_Top_Depth_predBy_NN1thick',\n",
    " 'HorID',\n",
    " 'Pick',\n",
    " 'Quality',\n",
    " 'HorID_paleoz',\n",
    " 'Pick_paleoz',\n",
    " 'Quality_paleoz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_all_wells_wKNN[columns_to_turn_to_floats].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we're going to find some depths! Most of the depths in the wells are from a column called DEPT but there a handfull of wells that use a column called DEPTH. For convience sake, we're going to move the DEPTH values were not NaN to the DEPT column so all depths are in the same column. We're also going to try to replace NaNs in GR with GR:1 and GR:2 where data exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useDiffColNamesToFillInNA(dataframeOfWells,colReplaceList):\n",
    "    \"\"\"\n",
    "    Takes in two arguments,\n",
    "    Argument one is a dataframe of multiple wells\n",
    "    Argument two is a list of lists. Where each sub-list is a  pair of column names. \n",
    "    The right col is used to fill in NANs where they exist in left column.\n",
    "    The function returns a dataframe of wells with the NANs in certain columns replaced based on input arguments.\n",
    "    Example = [[ColA,ColB],[ColF,ColG],[ColZ,ColE]]\n",
    "    \"\"\"\n",
    "    for each in colReplaceList:\n",
    "        print(\"each\",each)\n",
    "        dataframeOfWells[each[0]].fillna(dataframeOfWells[each[1]], inplace=True)\n",
    "    return dataframeOfWells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### list of sub-lists. Items on left are replaced with volumns from right column if left column has a NaN\n",
    "colReplaceList = [['DEPT','DEPTH'],['GR','GR:1'],['GR','GR:2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create new dataframe\n",
    "df_all_wells_wKNN_DEPTHtoDEPT = useDiffColNamesToFillInNA(df_all_wells_wKNN,colReplaceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Look at DEPT to make sure it has gone up, it has!\n",
    "df_all_wells_wKNN_DEPTHtoDEPT.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns for how close a row is (based on depth) from the official pick for that well. \n",
    "### We'll be doing this for Top and Base McMurray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for top McMurray\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'] = df_all_wells_wKNN_DEPTHtoDEPT['Pick'] - df_all_wells_wKNN_DEPTHtoDEPT['DEPT']\n",
    "#### for base McMurray or Top Paleozoic\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'] = df_all_wells_wKNN_DEPTHtoDEPT['Pick_paleoz'] - df_all_wells_wKNN_DEPTHtoDEPT['DEPT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### print a few wells to double check\n",
    "df_all_wells_wKNN_DEPTHtoDEPT[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT SHOULD BE NOTED THAT THE 'correct' PICK DEPTHS IN MANY CASES DO NOT PERFECTLY MATCH THE DEPTHS AVAILABLE IN THE LOGS. \n",
    "### In other words, the pick might be 105 but there is no row with 105.00 depth, only a 104.98 and a 105.02!\n",
    "### This matters for what you count as a correct label!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column for whether a row (based on depth) is within 0.0, +- 5, or >5 from the official pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a column that has a number that symbolizes whether a row is close or not to the 'real' pick\n",
    "#### We'll do this first for Top McMurray and then top Paleozoic, which is basically base McMurray\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopMcMrNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))\n",
    "#### Top paleozoic version\n",
    "df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopPalNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### drop previously created diff_TMcM_Pick_v_DEPT\n",
    "#df_all_wells_wKNN_DEPTHtoDEPT.drop(columns=['diff_Pick_v_DEPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### print a few wells to double check\n",
    "df_all_wells_wKNN_DEPTHtoDEPT.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use thickness from neighor and base to predict top just with that, add as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_df['new_pick']=l_df['Pick']-l_df['DEPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_wells_wKNN['diff_Pick_v_DEPT'] = df_all_wells_wKNN['Pick'] - df_all_wells_wKNN['DEPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT['MM_Top_Depth_predBy_NN1thick'][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Takes MM_Top_Depth_predBy_NN1thick and subtracts depth at that point, returns *absolute* value\n",
    "def NN1_TopMcMDepth_Abs(df,MM_Top_Depth_predBy_NN1thick):\n",
    "    df['DistFrom_NN1_TopDepth_Abs'] = abs(df[MM_Top_Depth_predBy_NN1thick] - df['DEPT'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM = NN1_TopMcMDepth_Abs(df_all_wells_wKNN_DEPTHtoDEPT,'MM_Top_Depth_predBy_NN1thick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we'll create a variety of calculated features based on well log numbers at, above, below, and around each depth point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The difficult thing about creating features based on windows within a well when you have multiple wells stacked in a dataframe is that sometimes that window from one well goes into the next well.\n",
    "\n",
    "#### To get around that, we're going create a column that says the distance from the top of the well and another column that says the distance form the bottom of the well. When a row's distance from top or bottom is greater than 1/2 the max window size, we'll just use proceed as normal. When the distance between that row's depth and top or bottom is less than 1/2 the max window size, we'll ....................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(-1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopOfWell'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM[NewWell] == True,,\n",
    "\n",
    "\n",
    "# df['elderly'] = np.where(df['age']>=50, 'yes', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopOfWellRowsOnly = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM.loc[df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] == True]\n",
    "BottomOfWellRowsOnly = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM.loc[df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename depth to top and bottom depths , delete all other columns\n",
    "TopOfWellRowsOnly = TopOfWellRowsOnly[['UWI','DEPT']]\n",
    "TopOfWellRowsOnly['TopWellDept'] = TopOfWellRowsOnly['DEPT']\n",
    "TopOfWellRowsOnly.drop(['DEPT'],axis=1, inplace=True)\n",
    "#### same thing for bottom\n",
    "BottomOfWellRowsOnly = BottomOfWellRowsOnly[['UWI','DEPT']]\n",
    "BottomOfWellRowsOnly['BotWellDept'] = BottomOfWellRowsOnly['DEPT']\n",
    "BottomOfWellRowsOnly.drop(['DEPT'],axis=1, inplace=True)\n",
    "#### merge these two small dataframes\n",
    "TopAndBottomOfWellRowsOnly = pd.merge(TopOfWellRowsOnly, BottomOfWellRowsOnly, on='UWI')\n",
    "#### merge with larger dataframe\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM = pd.merge(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM, TopAndBottomOfWellRowsOnly, on='UWI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a col for distance from row to top of well\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['DEPT'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopWellDept']\n",
    "\n",
    "#### Create a col for distance from row to bottom of well\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['BotWellDept'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['DEPT']\n",
    "\n",
    "#### Create col for well total thickness measured\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['WellThickness'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['BotWellDept'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopWellDept']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This adds a column that says whether a row is closer to the bottm or the top of the well\n",
    "#### This is useful for doing creation of features of rolling windows where you want to avoid going into another well stacked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This adds a column that says whether a row is closer to the bottm or the top of the well\n",
    "#### This is useful for doing creation of features of rolling windows where you want to avoid going into another well stacked above.\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closerToBotOrTop'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], 'FromTopWell', 'FromBotWell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist']/0.25\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing dataframe to pickle file before doing main feature creation step that uses curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dumping dict of data frame to pickle file\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM\n",
    "pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724, open( \"df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the same dataframe from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop =  pd.read_pickle('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is a rewrite of the basic features calculated from the curves\n",
    "### It runs faster than previous Pandas version could be made to run faster, specifically by using `apply` less\n",
    "### It also calculates things in a window around a point, above a point, but not below a point. I have to go back and re-write that code. Sorting is expensive task in Dask, so I don't want to reverse order twice for each feature like I did previously in Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next two lines bring up a Dask client dashboard that will open as a new tab. It provides great insight into what functions are being run by dask, how they run, and which ones are slowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit is only creating features based on curve data within a given well, so we'll read and write pickle files at the start and end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop =  pd.read_pickle('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "test_5 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop.copy()\n",
    "test_5 = dd.from_pandas(test_5, npartitions=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the line below, we pick the curves and windows to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves = ['GR','ILD']\n",
    "windows = [5,7,11,21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function nLargest is used via `apply`, I should probably re-write this to use Dask's Nlargest API but didn't here as the docs imply it might behave slightly differently.\n",
    "### A quick look at the status dashboard in the Dask Client suggests the use of apply takes up maybe 1/4-1/2 of total compute time currently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nLargest(array,nValues):\n",
    "    answer = np.mean(array[np.argsort(array)[-nValues:]])  \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thoughts_seperateRollingAndConditionalIntoTwoDaskProcesses(dd,curves,windows):\n",
    "    \"\"\"\n",
    "    for loop for each combination of parameter for rolling functions\n",
    "    curves = ['GR','ILD']\n",
    "    windows = [5,7,11,21]\n",
    "    directions = [\"around\",\"below\",\"above\"]\n",
    "        #         Not sure the best way to do the 'below' centered rolling in dask as the sort_index is expensive in dask so might be slow!\n",
    "        #       Skipping this for now will come back when not tired. Maybe use shift?\n",
    "    For each column created, check window size vs. allowable window size column, if too small, use single row value from original column\n",
    "    \"\"\"\n",
    "    comboArg_B = [curves,windows]\n",
    "    all_comboArgs_B = list(itertools.product(*comboArg_B))\n",
    "    for eachArgList in all_comboArgs_B:\n",
    "        col = eachArgList[0]\n",
    "        windowSize = eachArgList[1]\n",
    "        #centered = eachArgList[2]\n",
    "        featureName = col+\"_min_\"+str(windowSize)+\"winSize_\"\n",
    "        half_window = int(windowSize/2)\n",
    "        #         quarter_window = int(windowSize/4)\n",
    "\n",
    "        \n",
    "        ### goes through distance to edge and when less than windowSize writes \"too close\" otherwise returns NaN\n",
    "        ### fills in Nan with calculated feature column\n",
    "        ### replaces \"too close\" with NaN\n",
    "        ### replaces NaN with dd[col]\n",
    "        ### overrights original column\n",
    "        \n",
    "        #### MIN\n",
    "        dd[featureName+'dir'+'Around'+'Min'] = dd[col].rolling(windowSize,center=True).min()\n",
    "        dd[featureName+'dir'+'Around'+'Min'] = dd[featureName+'dir'+'Around'+'Min'].where(cond=dd['closTopBotDist'] > half_window, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Above'+'Min'] = dd[col].rolling(windowSize,center=False).min()\n",
    "        dd[featureName+'dir'+'Above'+'Min'] = dd[featureName+'dir'+'Above'+'Min'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "        #### MAX\n",
    "        dd[featureName+'dir'+'Around'+'Max'] = dd[col].rolling(windowSize,center=True).max()\n",
    "        dd[featureName+'dir'+'Around'+'Max'] = dd[featureName+'dir'+'Around'+'Max'].where(cond=dd['closTopBotDist'] > half_window, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Above'+'Max'] = dd[col].rolling(windowSize,center=False).max()\n",
    "        dd[featureName+'dir'+'Above'+'Max'] = dd[featureName+'dir'+'Above'+'Max'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "        #### Mean\n",
    "        dd[featureName+'dir'+'Around'+'Mean'] = dd[col].rolling(windowSize,center=True).mean()\n",
    "        dd[featureName+'dir'+'Around'+'Mean'] = dd[featureName+'dir'+'Around'+'Mean'].where(cond=dd['closTopBotDist'] > half_window, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Above'+'Mean'] = dd[col].rolling(windowSize,center=False).mean()\n",
    "        dd[featureName+'dir'+'Above'+'Mean'] = dd[featureName+'dir'+'Above'+'Mean'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "\n",
    "        ## nLargest\n",
    "        nValues = 5\n",
    "        dd[featureName+'dir'+'Above'+'nLarge'] = dd[col].rolling(windowSize,center=False).apply( lambda x: nLargest(x,nValues),raw=True)  \n",
    "        dd[featureName+'dir'+'Above'+'nLarge'] = dd[featureName+'dir'+'Above'+'nLarge'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "        \n",
    "        dd[featureName+'dir'+'Around'+'nLarge'] = dd[col].rolling(windowSize,center=True).apply(lambda x: nLargest(x,nValues),raw=True) \n",
    "        dd[featureName+'dir'+'Around'+'nLarge'] = dd[featureName+'dir'+'Around'+'nLarge'].where(cond=dd['closTopBotDist'] > windowSize, other=dd[col])\n",
    "    \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "ddf_test5 = thoughts_seperateRollingAndConditionalIntoTwoDaskProcesses(test_5,curves,windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "test5result = ddf_test5.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "test5result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test5result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test5result.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing pandas dataframe to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dumping dict of data frame to pickle file\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726 = test5result\n",
    "pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726, open( \"df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2 =  pd.read_pickle(\"df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pandas dataframe to hdf5 \n",
    "#### Dropping [Neighbors_Obj] col as it is object and can't be written to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write hdf5 to current directory\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2.drop(['Neighbors_Obj'], axis=1)\n",
    "filename = \"df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724\"\n",
    "ending = \".h5\"\n",
    "groupkey = \"a\"\n",
    "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3.to_hdf(filename+ending, key='/' + groupkey, format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore adding features based on map position using widget to draw polygons which are then one-hot encoded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any other features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createFeat_20180707_vA-Copy1.ipynb\r\n",
      "createFeat_20180707_vA.ipynb\r\n",
      "createFeat_20180707_vB.ipynb\r\n",
      "createFeat_20180707_vC.ipynb\r\n",
      "createFeat_20180725_vD.ipynb\r\n",
      "\u001b[34mdask-worker-space\u001b[m\u001b[m\r\n",
      "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724\r\n",
      "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p\r\n",
      "dict_of__wells_df_No_features_class3_20180707.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
